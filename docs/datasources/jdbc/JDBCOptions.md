# JDBCOptions

`JDBCOptions` is the options of the [JDBC](index.md) data source.

## Options

[[options]]
.Options for JDBC Data Source
[cols="1m,1,2",options="header",width="100%",separator="!"]
|===
! Option / Key
! Default Value
! Description

! batchsize
! `1000`
! [[batchsize]]

The minimum value is `1`

Used exclusively when `JdbcRelationProvider` is requested to [write the rows of a structured query (a DataFrame) to a table](JdbcRelationProvider.md#createRelation-CreatableRelationProvider) through `JdbcUtils` helper object and its <<spark-sql-JdbcUtils.md#saveTable, saveTable>>.

! createTableColumnTypes
!
! [[createTableColumnTypes]]

Used exclusively when `JdbcRelationProvider` is requested to [write the rows of a structured query (a DataFrame) to a table](JdbcRelationProvider.md#createRelation-CreatableRelationProvider) through `JdbcUtils` helper object and its <<spark-sql-JdbcUtils.md#createTable, createTable>>.

! `createTableOptions`
! Empty string
! [[createTableOptions]]

Used exclusively when `JdbcRelationProvider` is requested to [write the rows of a structured query (a DataFrame) to a table](JdbcRelationProvider.md#createRelation-CreatableRelationProvider) through `JdbcUtils` helper object and its <<spark-sql-JdbcUtils.md#createTable, createTable>>.

! `customSchema`
! (undefined)
a! [[customSchema]] Specifies the custom data types of the read schema (that is used at [load time](../../DataFrameReader.md#jdbc))

`customSchema` is a comma-separated list of field definitions with column names and their [DataType](../../DataType.md)s in a canonical SQL representation, e.g. `id DECIMAL(38, 0), name STRING`.

`customSchema` defines the data types of the columns that will override the data types inferred from the table schema and follows the following pattern:

```text
colTypeList
    : colType (',' colType)*
    ;

colType
    : identifier dataType (COMMENT STRING)?
    ;

dataType
    : complex=ARRAY '<' dataType '>'                            #complexDataType
    | complex=MAP '<' dataType ',' dataType '>'                 #complexDataType
    | complex=STRUCT ('<' complexColTypeList? '>' | NEQ)        #complexDataType
    | identifier ('(' INTEGER_VALUE (',' INTEGER_VALUE)* ')')?  #primitiveDataType
    ;
```

Used exclusively when `JDBCRelation` is requested for the <<datasources/jdbc/JDBCRelation.md#schema, schema>>.

! `dbtable`
!
a! [[dbtable]] (*required*)

Used when:

* `JDBCRDD` is requested to [resolveTable](JDBCRDD.md#resolveTable) (when `JDBCRelation` is requested for the <<datasources/jdbc/JDBCRelation.md#schema, schema>>) and <<datasources/jdbc/JDBCRelation.md#compute, compute a partition>>

* `JDBCRelation` is requested to <<datasources/jdbc/JDBCRelation.md#insert, insert or overwrite data>> and for the <<datasources/jdbc/JDBCRelation.md#toString, human-friendly text representation>>

* `JdbcRelationProvider` is requested to [write the rows of a structured query (a DataFrame) to a table](JdbcRelationProvider.md#createRelation-CreatableRelationProvider)

* `JdbcUtils` is requested to <<spark-sql-JdbcUtils.md#tableExists, tableExists>>, <<spark-sql-JdbcUtils.md#truncateTable, truncateTable>>, <<spark-sql-JdbcUtils.md#getSchemaOption, getSchemaOption>>, <<spark-sql-JdbcUtils.md#saveTable, saveTable>> and <<spark-sql-JdbcUtils.md#createTable, createTable>>

* `JDBCOptions` is <<creating-instance, created>> (with the input parameters for the <<url, url>> and <<dbtable, dbtable>> options)

* `DataFrameReader` is requested to [load data from external table using JDBC data source](../../DataFrameReader.md#jdbc) (using `DataFrameReader.jdbc` method with the input parameters for the <<url, url>> and <<dbtable, dbtable>> options)

! `driver`
!
a! [[driver]][[driverClass]] (*recommended*) Class name of the JDBC driver to use

Used exclusively when `JDBCOptions` is <<creating-instance, created>>. When the `driver` option is defined, the JDBC driver class will get registered with Java's https://docs.oracle.com/javase/8/docs/api/java/sql/DriverManager.html[java.sql.DriverManager].

NOTE: `driver` takes precedence over the class name of the driver for the <<url, url>> option.

After the JDBC driver class was registered, the driver class is used exclusively when `JdbcUtils` helper object is requested to <<spark-sql-JdbcUtils.md#createConnectionFactory, createConnectionFactory>>.

! `fetchsize`
! `0`
! [[fetchsize]] Hint to the JDBC driver as to the number of rows that should be fetched from the database when more rows are needed for `ResultSet` objects generated by a `Statement`

The minimum value is `0` (which tells the JDBC driver to do the estimates)

Used exclusively when `JDBCRDD` is requested to [compute a partition](JDBCRDD.md#compute).

! `isolationLevel`
! `READ_UNCOMMITTED`
a! [[isolationLevel]] One of the following:

* NONE
* READ_UNCOMMITTED
* READ_COMMITTED
* REPEATABLE_READ
* SERIALIZABLE

Used exclusively when `JdbcUtils` is requested to <<spark-sql-JdbcUtils.md#saveTable, saveTable>>.

! `lowerBound`
!
! [[lowerBound]] Lower bound of partition column

Used exclusively when `JdbcRelationProvider` is requested to [create a BaseRelation](JdbcRelationProvider.md#createRelation-RelationProvider) for reading

! `numPartitions`
!
a! [[numPartitions]] Number of partitions to use for loading or saving data

Used when:

* `JdbcRelationProvider` is requested to [loading data from a table using JDBC](JdbcRelationProvider.md#createRelation-RelationProvider)

* `JdbcUtils` is requested to <<spark-sql-JdbcUtils.md#saveTable, saveTable>>

! `partitionColumn`
!
! [[partitionColumn]] Name of the column used to partition dataset (using a `JDBCPartitioningInfo`).

Used exclusively when `JdbcRelationProvider` is requested to [create a BaseRelation](JdbcRelationProvider.md#createRelation-RelationProvider) for reading (with proper `JDBCPartitions` with `WHERE` clause)

When defined, the <<lowerBound, lowerBound>>, <<upperBound, upperBound>> and <<numPartitions, numPartitions>> options are also required.

When undefined, <<lowerBound, lowerBound>> and <<upperBound, upperBound>> have to be undefined.

! `truncate`
! `false`
! [[truncate]][[isTruncate]] (used only for writing) Enables table truncation

Used exclusively when `JdbcRelationProvider` is requested to [write the rows of a structured query (a DataFrame) to a table](JdbcRelationProvider.md#createRelation-CreatableRelationProvider)

! `sessionInitStatement`
!
! [[sessionInitStatement]] A generic SQL statement (or PL/SQL block) executed before reading a table/query

Used exclusively when `JDBCRDD` is requested to [compute a partition](JDBCRDD.md#compute).

! `upperBound`
!
! [[upperBound]] Upper bound of the partition column

Used exclusively when `JdbcRelationProvider` is requested to [create a BaseRelation](JdbcRelationProvider.md#createRelation-RelationProvider) for reading

! `url`
!
! [[url]] (*required*) A JDBC URL to use to connect to a database
|===

NOTE: The <<options, options>> are case-insensitive.

`JDBCOptions` is <<creating-instance, created>> when:

* `DataFrameReader` is requested to [load data from an external table using JDBC](../../DataFrameReader.md#jdbc) (and create a `DataFrame` to represent the process of loading the data)

* `JdbcRelationProvider` is requested to create a `BaseRelation` (as a [RelationProvider](JdbcRelationProvider.md#createRelation-RelationProvider) for loading and a [CreatableRelationProvider](JdbcRelationProvider.md#createRelation-CreatableRelationProvider) for writing)

## Creating Instance

`JDBCOptions` takes the following to be created:

* JDBC URL
* [[table]] Name of the table
* [[parameters]] Case-insensitive configuration parameters (i.e. `Map[String, String]`)

The input `URL` and <<table, table>> are set as the current <<url, url>> and <<dbtable, dbtable>> options (overriding the values in the input <<parameters, parameters>> if defined).

=== [[asProperties]] Converting Parameters (Options) to Java Properties -- `asProperties` Property

[source, scala]
----
asProperties: Properties
----

`asProperties`...FIXME

`asProperties` is used when:

* `JDBCRDD` is requested to [compute a partition](JDBCRDD.md#compute) (that requests a `JdbcDialect` to [beforeFetch](JdbcDialect.md#beforeFetch))

* `JDBCRelation` is requested to [insert a data (from a DataFrame) to a table](JDBCRelation.md#insert)

=== [[asConnectionProperties]] `asConnectionProperties` Property

[source, scala]
----
asConnectionProperties: Properties
----

`asConnectionProperties`...FIXME

NOTE: `asConnectionProperties` is used exclusively when `JdbcUtils` is requested to spark-sql-JdbcUtils.md#createConnectionFactory[createConnectionFactory]
