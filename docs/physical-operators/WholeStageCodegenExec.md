---
title: WholeStageCodegenExec
---

# WholeStageCodegenExec Physical Operator

`WholeStageCodegenExec` is a [unary physical operator](UnaryExecNode.md) that (alongside [InputAdapter](InputAdapter.md)) lays the foundation for the [Whole-Stage Java Code Generation](../whole-stage-code-generation/index.md) for a **Codegened Execution Pipeline** of a structured query.

## Creating Instance

`WholeStageCodegenExec` takes the following to be created:

* <span id="child"> Child [SparkPlan](SparkPlan.md) (a physical subquery tree)
* <span id="codegenStageId"> Codegen Stage Id

`WholeStageCodegenExec` is created when:

* [CollapseCodegenStages](../physical-optimizations/CollapseCodegenStages.md) physical optimization is [executed](../physical-optimizations/CollapseCodegenStages.md#insertWholeStageCodegen) (with [spark.sql.codegen.wholeStage](../configuration-properties.md#spark.sql.codegen.wholeStage) configuration property enabled)

## <span id="CodegenSupport"> CodegenSupport

`WholeStageCodegenExec` is a [CodegenSupport](CodegenSupport.md) and, when [executed](#doExecute), triggers code generation for the whole child physical plan subtree (_stage_) of a structured query.

## <span id="metrics"> Performance Metrics

### <span id="pipelineTime"> duration

How long (in ms) the whole-stage codegen pipeline has been running (i.e. the elapsed time since the underlying [BufferedRowIterator](../whole-stage-code-generation/BufferedRowIterator.md) had been created and the internal rows were all consumed).

![WholeStageCodegenExec in web UI (Details for Query)](../images/spark-sql-WholeStageCodegenExec-webui.png)

## <span id="doExecute"> Executing Physical Operator

```scala
doExecute(): RDD[InternalRow]
```

`doExecute` is part of the [SparkPlan](SparkPlan.md#doExecute) abstraction.

---

`doExecute` [doCodeGen](#doCodeGen) (that gives a [CodegenContext](../whole-stage-code-generation/CodegenContext.md) and the Java source code).

`doExecute` tries to [compile the code](../whole-stage-code-generation/CodeGenerator.md#compile) and, if fails and [spark.sql.codegen.fallback](../configuration-properties.md#spark.sql.codegen.fallback) is enabled, falls back to requesting the [child](#child) physical operator to [execute](SparkPlan.md#execute) (and skipping codegen for the part of query).

In case the compiled code has the maximum bytecode size of a single compiled Java function (generated by whole-stage codegen) above [spark.sql.codegen.hugeMethodLimit](../configuration-properties.md#spark.sql.codegen.hugeMethodLimit) threshold, `doExecute` prints out the following INFO message and requests the [child](#child) physical operator to [execute](SparkPlan.md#execute) (and skipping codegen for the part of query):

```text
Found too long generated codes and JIT optimization might not work:
the bytecode size ([maxMethodCodeSize]) is above the limit [hugeMethodLimit],
and the whole-stage codegen was disabled for this plan (id=[codegenStageId]).
To avoid this, you can raise the limit `spark.sql.codegen.hugeMethodLimit`:
[treeString]
```

`doExecute` requests the `CodegenContext` for the [references](../whole-stage-code-generation/CodegenContext.md#references).

`doExecute` the [child](#child) physical operator (that is supposed to be a [CodegenSupport](CodegenSupport.md)) for the [inputRDDs](CodegenSupport.md#inputRDDs).

??? note "Up to two input RDDs are supported"
    `doExecute` throws an `AssertionError` when the number of input RDDs is more than 2:

    ```text
    Up to two input RDDs can be supported
    ```

### <span id="doExecute-one-input-rdd"> One Input RDD

For one input RDD, `doExecute` uses `RDD.mapPartitionsWithIndex` operation.

For every partition of the input RDD, `doExecute` does the following:

1. [Compiles the code](../whole-stage-code-generation/CodeGenerator.md#compile) (the compile code is cached the first time so it's almost a noop)
1. Requests the compiled code to `generate` (with the [references](../whole-stage-code-generation/CodegenContext.md#references)) to produce a [BufferedRowIterator](../whole-stage-code-generation/BufferedRowIterator.md)
1. Requests the `BufferedRowIterator` to [initialize](../whole-stage-code-generation/BufferedRowIterator.md#init)
1. Creates an `Iterator[InternalRow]` to track the rows until the last is consumed and the [duration](#pipelineTime) metric can be recorded

### <span id="doExecute-one-input-rdd"> Two Input RDDs

For two input RDDs, `doExecute`...FIXME

## Demo

```scala
val q = spark.range(9)

// we need executedPlan with WholeStageCodegenExec physical operator "injected"
val plan = q.queryExecution.executedPlan
assert(plan.isInstanceOf[org.apache.spark.sql.execution.SparkPlan])
```

```text
// Note the star prefix of Range that marks WholeStageCodegenExec
scala> println(plan.numberedTreeString)
00 *Range (0, 9, step=1, splits=8)
```

```text
// As a matter of fact, there are two physical operators in play here
// i.e. WholeStageCodegenExec with Range as the child
scala> plan.foreach { op => println(op.getClass.getName) }
org.apache.spark.sql.execution.WholeStageCodegenExec
org.apache.spark.sql.execution.RangeExec
```

```scala
// Let's access the parent WholeStageCodegenExec
import org.apache.spark.sql.execution.WholeStageCodegenExec
val wsce = plan.asInstanceOf[WholeStageCodegenExec]
```

```scala
// Trigger code generation of the entire query plan tree
val (ctx, code) = wsce.doCodeGen
```

```scala
// CodeFormatter can pretty-print the code
import org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatter
println(CodeFormatter.format(code))
```

```text
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean range_initRange_0;
/* 010 */   private long range_nextIndex_0;
/* 011 */   private TaskContext range_taskContext_0;
/* 012 */   private InputMetrics range_inputMetrics_0;
/* 013 */   private long range_batchEnd_0;
/* 014 */   private long range_numElementsTodo_0;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] range_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 016 */
/* 017 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 018 */     this.references = references;
/* 019 */   }
/* 020 */
/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 022 */     partitionIndex = index;
/* 023 */     this.inputs = inputs;
/* 024 */
...
```

```scala
import org.apache.spark.sql.catalyst.expressions.codegen._

val ctx = new CodegenContext()

val code: Block = CodeBlock(codeParts = Seq("valid_java_code"), blockInputs = Seq.empty)

// code will be evaluated to produce a value (that can be null)
val isNull: ExprValue = FalseLiteral
val value: ExprValue = new LiteralValue(value = "valid_java_code_for_literal_value", javaType = classOf[String])
val exprCode = ExprCode(code, isNull, value)

val consumeCode = wsce.doConsume(ctx, input = Seq.empty, row = exprCode)
```

```scala
println(consumeCode)
```

```text
valid_java_code
append(valid_java_code_for_literal_value);
```

## Logging

Enable `ALL` logging level for `org.apache.spark.sql.execution.WholeStageCodegenExec` logger to see what happens inside.

Add the following line to `conf/log4j2.properties`:

```text
log4j.logger.org.apache.spark.sql.execution.WholeStageCodegenExec=ALL
```

Refer to [Logging](../spark-logging.md).

<!---
## Review Me

[TIP]
====
Use the following to enable comments in generated code.

[source, scala]
----
org.apache.spark.SparkEnv.get.conf.set("spark.sql.codegen.comments", "true")
----
====

[source, scala]
----
val q = spark.range(10).where('id === 4)
import org.apache.spark.sql.execution.debug._
scala> q.debugCodegen()
Found 1 WholeStageCodegen subtrees.
== Subtree 1 / 1 ==
*(1) Filter (id#6L = 4)
+- *(1) Range (0, 10, step=1, splits=8)

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ /**
 * Codegend pipeline for stage (id=1)
 * *(1) Filter (id#6L = 4)
 * +- *(1) Range (0, 10, step=1, splits=8)
 */
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
...
----

NOTE: `WholeStageCodegenExec` <<doCodeGen, requires>> that the single <<child, child>> physical operator [supports Java code generation](CodegenSupport.md).

```text
// RangeExec physical operator does support codegen
import org.apache.spark.sql.execution.RangeExec
import org.apache.spark.sql.catalyst.plans.logical.Range
val rangeExec = RangeExec(Range(start = 0, end = 1, step = 1, numSlices = 1))

import org.apache.spark.sql.execution.WholeStageCodegenExec
val rdd = WholeStageCodegenExec(rangeExec)(codegenStageId = 0).execute()
```

[[generateTreeString]]
`WholeStageCodegenExec` marks the <<child, child>> physical operator with `*` (star) prefix and <<codegenStageId, per-query codegen stage ID>> (in round brackets) in the [text representation of a physical plan tree](../catalyst/TreeNode.md#generateTreeString).

```text
scala> println(plan.numberedTreeString)
00 *(1) Project [id#117L]
01 +- *(1) BroadcastHashJoin [id#117L], [cast(id#115 as bigint)], Inner, BuildRight
02    :- *(1) Range (0, 1, step=1, splits=8)
03    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
04       +- Generate explode(ids#112), false, [id#115]
05          +- LocalTableScan [ids#112]
```

NOTE: As `WholeStageCodegenExec` is created as a result of [CollapseCodegenStages](../physical-optimizations/CollapseCodegenStages.md) physical optimization, it is only executed in [executedPlan](../QueryExecution.md#executedPlan) phase of a query execution (that you can only notice by the `*` star prefix in a plan output).

When <<doExecute, executed>>, `WholeStageCodegenExec` gives <<pipelineTime, pipelineTime>> performance metric.

TIP: Use Dataset.md#explain[explain] operator to know the physical plan of a query and find out whether or not `WholeStageCodegen` is in use.

[source, scala]
----
val q = spark.range(10).where('id === 4)
// Note the stars in the output that are for codegened operators
scala> q.explain
== Physical Plan ==
*Filter (id#0L = 4)
+- *Range (0, 10, step=1, splits=8)
----

=== [[doCodeGen]] Generating Java Source Code for Child Physical Plan Subtree -- `doCodeGen` Method

[source, scala]
----
doCodeGen(): (CodegenContext, CodeAndComment)
----

`doCodeGen` creates a new [CodegenContext](../whole-stage-code-generation/CodegenContext.md) and requests the single <<child, child>> physical operator to [generate a Java source code for produce code path](CodegenSupport.md#produce) (with the new `CodegenContext` and the `WholeStageCodegenExec` physical operator itself).

`doCodeGen` [adds the new function](../whole-stage-code-generation/CodegenContext.md#addNewFunction) under the name of `processNext`.

`doCodeGen` <<generatedClassName, generates the class name>>.

`doCodeGen` generates the final Java source code of the following format:

[source, scala]
----
public Object generate(Object[] references) {
  return new [className](references);
}

/**
 * Codegend pipeline for stage (id=[codegenStageId])
 * [treeString]
 */
final class [className] extends BufferedRowIterator {

  private Object[] references;
  private scala.collection.Iterator[] inputs;
  // ctx.declareMutableStates()

  public [className](Object[] references) {
    this.references = references;
  }

  public void init(int index, scala.collection.Iterator[] inputs) {
    partitionIndex = index;
    this.inputs = inputs;
    // ctx.initMutableStates()
    // ctx.initPartition()
  }

  // ctx.emitExtraCode()

  // ctx.declareAddedFunctions()
}
----

`doCodeGen` requires that the single [child](#child) physical operator [supports Java code generation](CodegenSupport.md).

`doCodeGen` cleans up the generated code (using `CodeFormatter` to `stripExtraNewLines`, `stripOverlappingComments`).

`doCodeGen` prints out the following DEBUG message to the logs:

```text
DEBUG WholeStageCodegenExec:
[cleanedSource]
```

In the end, `doCodeGen` returns the [CodegenContext](../whole-stage-code-generation/CodegenContext.md) and the Java source code (as a `CodeAndComment`).

`doCodeGen` is used when:

* `WholeStageCodegenExec` is <<doExecute, executed>>

* Debugging Query Execution is requested to [display a Java source code generated for a structured query in Whole-Stage Code Generation](../debugging-query-execution.md#debugCodegen)

## <span id="doConsume"> Generating Java Source Code for Consume Path

```scala
doConsume(
  ctx: CodegenContext,
  input: Seq[ExprCode],
  row: ExprCode): String
```

`doConsume` is part of the [CodegenSupport](CodegenSupport.md#doConsume) abstraction.

!!! danger
    Review Me

`doConsume` generates a Java source code that:

1. Takes (from the input `row`) the code to evaluate a Catalyst expression on an input `InternalRow`
1. Takes (from the input `row`) the term for a value of the result of the evaluation
  a. Adds `.copy()` to the term if <<needCopyResult, needCopyResult>> is turned on
1. Wraps the term inside `append()` code block

## <span id="generatedClassName"> Generating Class Name

```scala
generatedClassName(): String
```

`generatedClassName` gives a class name per [spark.sql.codegen.useIdInClassName](../configuration-properties.md#spark.sql.codegen.useIdInClassName) configuration property:

* `GeneratedIteratorForCodegenStage` with the <<codegenStageId, codegen stage ID>> when enabled (`true`)

* `GeneratedIterator` when disabled (`false`)

`generatedClassName` is used when `WholeStageCodegenExec` unary physical operator is requested to [generate the Java source code for the child physical plan subtree](#doCodeGen).
-->
