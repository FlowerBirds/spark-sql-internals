# SparkSession &mdash; The Entry Point to Spark SQL

`SparkSession` is the entry point to Spark SQL. It is one of the first objects created in a Spark SQL application.

`SparkSession` is [created](#creating-instance) using the [SparkSession.builder](#builder) method.

```scala
import org.apache.spark.sql.SparkSession
val spark = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // only for demo and testing purposes, use spark-submit instead
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .withExtensions { extensions =>
    extensions.injectResolutionRule { session =>
      ...
    }
    extensions.injectOptimizerRule { session =>
      ...
    }
  }
  .getOrCreate
```

`SparkSession` is a namespace of relational entities (e.g. databases, tables). A Spark SQL application could use many `SparkSessions` to keep the relational entities separate logically in [metadata catalogs](#catalog).

!!! note "SparkSession in spark-shell"
    `spark` object in `spark-shell` (the instance of `SparkSession` that is auto-created) has Hive support enabled.

    In order to disable the pre-configured Hive support in the `spark` object, use [spark.sql.catalogImplementation](StaticSQLConf.md#spark.sql.catalogImplementation) internal configuration property with `in-memory` value (that uses [InMemoryCatalog](InMemoryCatalog.md) external catalog instead).

    ```text
    $ spark-shell --conf spark.sql.catalogImplementation=in-memory
    ```

## Creating Instance

`SparkSession` takes the following to be created:

* <span id="sparkContext"> `SparkContext`
* <span id="existingSharedState"> Existing [SharedState](SharedState.md) (if given)
* <span id="parentSessionState"> Parent [SessionState](SessionState.md) (if given)
* <span id="extensions"> [SparkSessionExtensions](SparkSessionExtensions.md)

`SparkSession` is created when:

* `SparkSession.Builder` is requested to [getOrCreate](SparkSession-Builder.md#getOrCreate)
* Indirectly using [newSession](#newSession) or [cloneSession](#cloneSession)

## <span id="sessionState"> SessionState

```scala
sessionState: SessionState
```

`sessionState` is the current [SessionState](SessionState.md).

Internally, `sessionState` <<SessionState.md#clone, clones>> the optional <<parentSessionState, parent SessionState>> (if given when <<creating-instance, creating the SparkSession>>) or <<instantiateSessionState, creates a new SessionState>> using <<BaseSessionStateBuilder.md#, BaseSessionStateBuilder>> as defined by <<StaticSQLConf.md#spark.sql.catalogImplementation, spark.sql.catalogImplementation>> configuration property:

* *in-memory* (default) for SessionStateBuilder.md[org.apache.spark.sql.internal.SessionStateBuilder]
* *hive* for hive/HiveSessionStateBuilder.md[org.apache.spark.sql.hive.HiveSessionStateBuilder]

## <span id="newSession"> Creating New SparkSession

```scala
newSession(): SparkSession
```

`newSession` creates a new `SparkSession` with an undefined parent [SessionState](#parentSessionState) and (re)using the following:

* [SparkContext](#sparkContext)
* [SharedState](#sharedState)
* [SparkSessionExtensions](#extensions)

!!! note "SparkSession.newSession and SparkSession.cloneSession"
    `SparkSession.newSession` uses no parent [SessionState](#parentSessionState) while [SparkSession.cloneSession](#cloneSession) (re)uses [SessionState](#sessionState).

## <span id="cloneSession"> Cloning SparkSession

```scala
cloneSession(): SparkSession
```

`cloneSession`...FIXME

`cloneSession` is used when:

* `AdaptiveSparkPlanHelper` is requested to `getOrCloneSessionWithAqeOff`
* `StreamExecution` (Spark Structured Streaming) is created

## <span id="builder"> Creating SparkSession Using Builder Pattern

```scala
builder(): Builder
```

`builder` is an object method that creates a new [Builder](SparkSession-Builder.md) to build a `SparkSession` using a _fluent API_.

```scala
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
```

TIP: Read about https://en.wikipedia.org/wiki/Fluent_interface[Fluent interface] design pattern in Wikipedia, the free encyclopedia.

## <span id="version"> Spark Version

```scala
version: String
```

`version` returns the version of Apache Spark in use.

Internally, `version` uses `spark.SPARK_VERSION` value that is the `version` property in `spark-version-info.properties` properties file on CLASSPATH.

## <span id="emptyDataset"> Creating Empty Dataset (Given Encoder)

```scala
emptyDataset[T: Encoder]: Dataset[T]
```

`emptyDataset` creates an empty [Dataset](Dataset.md) (assuming that future records being of type `T`).

```text
scala> val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]

scala> strings.printSchema
root
 |-- value: string (nullable = true)
```

`emptyDataset` creates a [LocalRelation](logical-operators/LocalRelation.md) logical operator.

## <span id="createDataset"> Creating Dataset from Local Collections or RDDs

```scala
createDataset[T : Encoder](
  data: RDD[T]): Dataset[T]
createDataset[T : Encoder](
  data: Seq[T]): Dataset[T]
```

`createDataset` creates a [Dataset](Dataset.md) from a local Scala collection, i.e. `Seq[T]`, Java's `List[T]`, or a distributed `RDD[T]`.

```text
scala> val one = spark.createDataset(Seq(1))
one: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> one.show
+-----+
|value|
+-----+
|    1|
+-----+
```

`createDataset` creates logical operators:

* [LocalRelation](logical-operators/LocalRelation.md) for the input `data` collection
* [LogicalRDD](logical-operators/LogicalRDD.md) for the input `RDD[T]`


!!! tip "implicits object"
    You may want to consider [implicits](implicits.md) object and `toDS` method instead.

    ```text
    val spark: SparkSession = ...
    import spark.implicits._

    scala> val one = Seq(1).toDS
    one: org.apache.spark.sql.Dataset[Int] = [value: int]
    ```

Internally, `createDataset` first looks up the implicit [ExpressionEncoder](ExpressionEncoder.md) in scope to access the ``AttributeReference``s (of the [schema](types/index.md)).

The expression encoder is then used to map elements (of the input `Seq[T]`) into a collection of [InternalRow](InternalRow.md)s. With the references and rows, `createDataset` returns a Dataset.md[Dataset] with a LocalRelation.md[`LocalRelation` logical query plan].

## <span id="range"> Creating Dataset With Single Long Column

```scala
range(end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long]
```

`range` method family create a [Dataset](Dataset.md) of `Long` numbers.

```text
scala> spark.range(start = 0, end = 4, step = 2, numPartitions = 5).show
+---+
| id|
+---+
|  0|
|  2|
+---+
```

The three first variants (that do not specify `numPartitions` explicitly) use `SparkContext.defaultParallelism` for the number of partitions.

Internally, `range` creates a new `Dataset[Long]` with [Range](logical-operators/Range.md) leaf logical operator and `Encoders.LONG` encoder.

## <span id="sql"> Executing SQL Queries (aka SQL Mode)

```scala
sql(sqlText: String): DataFrame
```

`sql` executes the `sqlText` SQL statement and creates a [DataFrame](DataFrame.md).

!!! note spark-shell
    `sql` is imported in `spark-shell` so you can execute SQL statements as if `sql` were a part of the environment.

    ```text
    scala> :imports
    1) import spark.implicits._       (72 terms, 43 are implicit)
    2) import spark.sql               (1 terms)
    ```

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

Internally, `sql` requests the SessionState.md#sqlParser[current `ParserInterface`] to spark-sql-ParserInterface.md#parsePlan[execute a SQL query] that gives a spark-sql-LogicalPlan.md[LogicalPlan].

NOTE: `sql` uses `SessionState` SessionState.md#sqlParser[to access the current `ParserInterface`].

`sql` then creates a [DataFrame](DataFrame.md) using the current `SparkSession` (itself) and the [LogicalPlan](logical-operators/LogicalPlan.md).

!!! tip "spark-sql Command-Line Tool"
    Use [spark-sql](tools/spark-sql-spark-sql.md) command-line tool to use SQL directly (not Scala as in `spark-shell`).

    ```text
    spark-sql> show databases;
    default
    Time taken: 0.028 seconds, Fetched 1 row(s)
    ```

## <span id="udf"> Accessing UDFRegistration

```scala
udf: UDFRegistration
```

`udf` attribute is [UDFRegistration](UDFRegistration.md) (for registering [user-defined functions](spark-sql-udfs.md) for SQL-based queries).

```text
val spark: SparkSession = ...
spark.udf.register("myUpper", (s: String) => s.toUpperCase)

val strs = ('a' to 'c').map(_.toString).toDS
strs.registerTempTable("strs")

scala> sql("SELECT *, myUpper(value) UPPER FROM strs").show
+-----+-----+
|value|UPPER|
+-----+-----+
|    a|    A|
|    b|    B|
|    c|    C|
+-----+-----+
```

Internally, it is simply an alias for [SessionState.udfRegistration](SessionState.md#udfRegistration).

## <span id="table"> Loading Data From Table

```scala
table(
  multipartIdentifier: Seq[String]): DataFrame
table(
  tableName: String): DataFrame
table(
  tableIdent: TableIdentifier): DataFrame
```

`table` creates a [DataFrame](DataFrame.md) for the input `tableName` table.

!!! note
    [baseRelationToDataFrame](#baseRelationToDataFrame) acts as a mechanism to plug `BaseRelation` object hierarchy in into adoc[LogicalPlan](logical-operators/LogicalPlan.md) object hierarchy that `SparkSession` uses to bridge them.

```text
scala> spark.catalog.tableExists("t1")
res1: Boolean = true

// t1 exists in the catalog
// let's load it
val t1 = spark.table("t1")
```

## Catalog

```scala
catalog: Catalog
```

`catalog` creates a [CatalogImpl](CatalogImpl.md) when first accessed.

??? note "lazy value"
    `catalog` is a Scala lazy value which is computed once when accessed and cached afterwards.

## <span id="read"> DataFrameReader

```scala
read: DataFrameReader
```

`read` gives [DataFrameReader](DataFrameReader.md) to load data from external data sources and load it into a `DataFrame`.

```scala
val spark: SparkSession = ... // create instance
val dfReader: DataFrameReader = spark.read
```

## <span id="conf"> Runtime Configuration

```scala
conf: RuntimeConfig
```

`conf` returns the current [RuntimeConfig](RuntimeConfig.md).

Internally, `conf` creates a [RuntimeConfig](RuntimeConfig.md) (when requested the very first time and cached afterwards) with the [SQLConf](SessionState.md#conf) (of the [SessionState](#sessionState)).

## <span id="experimentalMethods"> ExperimentalMethods

```scala
experimental: ExperimentalMethods
```

`experimentalMethods` is an extension point with [ExperimentalMethods](ExperimentalMethods.md) that is a per-session collection of extra strategies and ``Rule[LogicalPlan]``s.

`experimental` is used in [SparkPlanner](SparkPlanner.md) and [SparkOptimizer](SparkOptimizer.md).

## <span id="baseRelationToDataFrame"> Create DataFrame for BaseRelation

```scala
baseRelationToDataFrame(
  baseRelation: BaseRelation): DataFrame
```

Internally, `baseRelationToDataFrame` creates a [DataFrame](DataFrame.md) from the input [BaseRelation](BaseRelation.md) wrapped inside [LogicalRelation](logical-operators/LogicalRelation.md).

`baseRelationToDataFrame` is used when:

* `DataFrameReader` is requested to load data from [data source](DataFrameReader.md#load) or [JDBC table](DataFrameReader.md#jdbc)
* `TextInputCSVDataSource` creates a base `Dataset` (of Strings)
* `TextInputJsonDataSource` creates a base `Dataset` (of Strings)

## <span id="instantiateSessionState"> Creating SessionState

```scala
instantiateSessionState(
  className: String,
  sparkSession: SparkSession): SessionState
```

`instantiateSessionState` finds the `className` that is then used to [create](BaseSessionStateBuilder.md#creating-instance) and [build](BaseSessionStateBuilder.md#build) a `BaseSessionStateBuilder`.

`instantiateSessionState` may report an `IllegalArgumentException` while instantiating the class of a `SessionState`:

```text
Error while instantiating '[className]'
```

`instantiateSessionState` is used when `SparkSession` is requested for [SessionState](#sessionState) (based on [spark.sql.catalogImplementation](StaticSQLConf.md#spark.sql.catalogImplementation) configuration property).

## <span id="sessionStateClassName"> sessionStateClassName

```scala
sessionStateClassName(
  conf: SparkConf): String
```

`sessionStateClassName` gives the name of the class of the [SessionState](SessionState.md) per [spark.sql.catalogImplementation](StaticSQLConf.md#spark.sql.catalogImplementation), i.e.

* [org.apache.spark.sql.hive.HiveSessionStateBuilder](hive/HiveSessionStateBuilder.md) for `hive`
* [org.apache.spark.sql.internal.SessionStateBuilder](SessionStateBuilder.md) for `in-memory`

`sessionStateClassName` is used when `SparkSession` is requested for the [SessionState](#sessionState) (and one is not available yet).

## <span id="internalCreateDataFrame"> Creating DataFrame From RDD Of Internal Binary Rows and Schema

```scala
internalCreateDataFrame(
  catalystRows: RDD[InternalRow],
  schema: StructType,
  isStreaming: Boolean = false): DataFrame
```

`internalCreateDataFrame` creates a [DataFrame](Dataset.md#ofRows) with [LogicalRDD](logical-operators/LogicalRDD.md).

`internalCreateDataFrame` is used when:

* `DataFrameReader` is requested to create a DataFrame from Dataset of [JSONs](DataFrameReader.md#json) or [CSVs](DataFrameReader.md#csv)

* `SparkSession` is requested to [create a DataFrame from RDD of rows](#createDataFrame)

* [InsertIntoDataSourceCommand](logical-operators/InsertIntoDataSourceCommand.md) logical command is executed

## <span id="listenerManager"> ExecutionListenerManager

```scala
listenerManager: ExecutionListenerManager
```

[ExecutionListenerManager](ExecutionListenerManager.md)

## <span id="sharedState"> SharedState

```scala
sharedState: SharedState
```

[SharedState](SharedState.md)

## <span id="time"> Measuring Duration of Executing Code Block

```scala
time[T](f: => T): T
```

`time` executes a code block and prints out (to standard output) the time taken to execute it

## <span id="applyExtensions"> Applying SparkSessionExtensions

```scala
applyExtensions(
  extensionConfClassNames: Seq[String],
  extensions: SparkSessionExtensions): SparkSessionExtensions
```

For every extension class name (in `extensionConfClassNames`) `applyExtensions` instantiates it and (since it's a function `SparkSessionExtensions => Unit`) passes the given [SparkSessionExtensions](SparkSessionExtensions.md) in.

!!! note
    The given [SparkSessionExtensions](SparkSessionExtensions.md) is mutated in-place.

In case of `ClassCastException`, `ClassNotFoundException` or `NoClassDefFoundError`, `applyExtensions` prints out the following WARN message to the logs:

```text
Cannot use [extensionConfClassName] to configure session extensions.
```

`applyExtensions` is used when:

* `SparkSession.Builder` is requested to [get active or create a new SparkSession instance](SparkSession-Builder.md#getOrCreate)
* `SparkSession` is [created](#creating-instance) (from a `SparkContext`)

## <span id="leafNodeDefaultParallelism"> Default Parallelism of Leaf Nodes

```scala
leafNodeDefaultParallelism: Int
```

`leafNodeDefaultParallelism` is the value of [spark.sql.leafNodeDefaultParallelism](configuration-properties.md#spark.sql.leafNodeDefaultParallelism) if defined or `SparkContext.defaultParallelism` ([Spark Core]({{ book.spark_core }}/SparkContext#defaultParallelism)).

`leafNodeDefaultParallelism` is used when:

* `SparkSession` is requested to [range](SparkSession.md#range)
* `RangeExec` leaf physical operator is [created](physical-operators/RangeExec.md#numSlices)
* `CommandResultExec` physical operator is requested for the `RDD[InternalRow]`
* `LocalTableScanExec` physical operator is requested for the [RDD](physical-operators/LocalTableScanExec.md#rdd)
* `FilePartition` utility is used to `maxSplitBytes`
